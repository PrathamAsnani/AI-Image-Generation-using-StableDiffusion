# -*- coding: utf-8 -*-
"""image.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-S2hURox5hga4OO5k3iTdW5OVhiG4yQK
"""

# installing the libraries
!pip install git+https://github.com/huggingface/diffusers
!pip install transformers accelerate safetensors diffusers
!pip install gradio

# importing the libraries
from diffusers import StableDiffusionXLPipeline
import torch
from transformers.pipelines.image_to_text import Image
from IPython.display import display
import gradio as gr

# initiating the stable diffusion pipeline
pipe = StableDiffusionXLPipeline.from_pretrained(
    "segmind/SSD-1B",
    torch_dtype=torch.float16,
    use_safetensors=True,
    variant="fp16"
)
pipe.to("cuda")

import gradio as gr

def greet(name, intensity):
    return "Hello, " + name + "!" * int(intensity)

demo = gr.Interface(
    fn=greet,
    inputs=["text", "slider"],
    outputs=["text"],
)

demo.launch()

# prompts to generate image
prompt = "sports car"
neg_prompt = "ugly, darkness, blurry, poor quality"

image = pipe(prompt = prompt,
             negative_prompt = neg_prompt,
             width=1500 - (1500 % 8),
             height=750 - (750 % 8),
             guidance_scale=7,
             num_inference_steps=28).images[0]
display(image)

from diffusers import StableDiffusionXLPipeline
import torch
pipe = StableDiffusionXLPipeline.from_pretrained("segmind/SSD-1B", torch_dtype=torch.float16, use_safetensors=True, variant="fp16")
pipe.to("cuda")
# if using torch < 2.0
# pipe.enable_xformers_memory_efficient_attention()
prompt = "purple sun set on mountains" # Your prompt here
neg_prompt = "ugly, blurry, poor quality" # Negative prompt here
image = pipe(prompt=prompt, negative_prompt=neg_prompt).images[0]
display(image)

from diffusers import StableDiffusionXLPipeline
import torch
pipe = StableDiffusionXLPipeline.from_pretrained("segmind/SSD-1B", torch_dtype=torch.float16, use_safetensors=True, variant="fp16")
pipe.to("cuda")
# if using torch < 2.0
# pipe.enable_xformers_memory_efficient_attention()
prompt = "new york city skyline at night " # Your prompt here
neg_prompt = "poor quality" # Negative prompt here
image = pipe(prompt=prompt, negative_prompt=neg_prompt).images[0]
display(image)

import gradio as gr
from diffusers import StableDiffusionXLPipeline
import torch

pipe = StableDiffusionXLPipeline.from_pretrained(
    "segmind/SSD-1B",
    torch_dtype=torch.float16,
    use_safetensors=True,
    variant="fp16"
)
pipe.to("cuda")

def generate_image(prompt, neg_prompt):
    image = pipe(
        prompt=prompt,
        negative_prompt=neg_prompt
    ).images[0]
    return image

prompt = gr.Text(
                label="Prompt",
                show_label=False,
                max_lines=1,
                placeholder="Enter your prompt",
                container=False,
            )
neg_prompt = gr.Text(
                label="Negative Prompt",
                show_label=False,
                max_lines=1,
                placeholder="Enter your negative prompt",
                container=False,
            )

iface = gr.Interface(
    fn=generate_image,
    inputs=[prompt, neg_prompt],
    outputs="image",
    title="Text to Image Generation",
    examples=[
        ["a painting of a cute cat sitting on a chair", "ugly, blurry"],
        ["an astronaut riding a horse on mars", "poorly drawn"]
    ],
   flagging_mode="never" # Changed allow_flagging=False to flagging_mode="never"
)

iface.launch(share=True)